# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

PM Tools is a comprehensive A/B testing solution for Product Managers, consisting of two integrated components:

1. **FastAPI Backend** (`app/`) - Statistical calculations and LLM-powered analysis
2. **Chrome Extension** (`chrome-extension/`) - Browser-based UI for instant access

The project serves as a "statistical consultant in a box" providing:
- **Pre-Experiment Feasibility Calculator** - Validates experiment design before running tests
- **Post-Experiment Results Interpreter** - Analyzes results with AI-powered insights

### üéØ Standalone Chrome Extension (COMPLETED - December 2024)
The standalone Chrome extension is now fully implemented with:
- ‚úÖ All statistical calculations running client-side in JavaScript
- ‚úÖ BYOK (Bring Your Own Key) model with dynamic model selection
- ‚úÖ No server dependencies - works entirely in the browser
- ‚úÖ Direct API calls to Gemini and Anthropic
- ‚úÖ Enhanced statistical accuracy with proper formulas
- ‚úÖ PM-focused AI prompts for actionable insights
- ‚úÖ Real-time model discovery from API providers
- ‚úÖ 24-hour caching for model lists
- ‚úÖ Markdown to HTML conversion for LLM responses

## Architecture Overview

### Current Architecture (v1.0)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Chrome Extension   ‚îÇ <-----> ‚îÇ   FastAPI Backend    ‚îÇ
‚îÇ  (UI + API Client)  ‚îÇ  HTTP   ‚îÇ (Stats + LLM + API)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Standalone Chrome Extension Architecture (v2.0 - LIVE)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Standalone Chrome Extension        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  UI Layer (popup.html/js)   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Statistics Engine (JS)      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Sample size calculation   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Significance testing      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Hart's algorithm (CDF)    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  LLM Client (BYOK)           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Dynamic model selection   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Direct API calls          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Fallback handling         ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Development Commands

### Backend (FastAPI) Commands

#### Setup and Installation
```bash
# Install dependencies
uv sync

# Install development dependencies
uv sync --dev
```

#### Running the API Server
```bash
# Run development server with auto-reload
uv run uvicorn app.main:app --reload

# Run on specific host/port
uv run uvicorn app.main:app --host 0.0.0.0 --port 8000

# Run in production mode
uv run uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Chrome Extension Commands

#### Installation (Standalone Version)
```bash
# 1. Open Chrome and navigate to:
chrome://extensions/

# 2. Enable "Developer mode" (top right)

# 3. Click "Load unpacked" and select:
/path/to/pmtools/chrome-extension-standalone/

# 4. Pin the extension to toolbar for easy access

# 5. Click extension icon ‚Üí Settings (‚öôÔ∏è) to add API keys
```

#### Development Workflow
```bash
# 1. Edit extension files
# 2. Go to chrome://extensions/
# 3. Click refresh icon on PM Tools extension
# 4. Test changes in extension popup

# For background script debugging:
# chrome://extensions/ ‚Üí PM Tools ‚Üí "Inspect views: service worker"

# For popup debugging:
# Right-click extension icon ‚Üí "Inspect popup"
```

### Testing

#### Backend Tests
```bash
# Run all tests
uv run pytest

# Run tests with coverage
uv run pytest --cov=app

# Run specific test file
uv run pytest tests/test_statistics.py

# Run specific test
uv run pytest tests/test_api.py::TestValidateSetupEndpoint::test_valid_setup_request_relative_mde
```

#### Chrome Extension Testing
```bash
# Manual testing checklist:
# ‚úì Extension loads without errors
# ‚úì Form inputs have name attributes (required for FormData)
# ‚úì Form validation catches errors
# ‚úì Statistical calculations are accurate
# ‚úì Results display correctly with formatted markdown
# ‚úì Auto-scroll to results works
# ‚úì Export functionality works (CSV/JSON)
# ‚úì Settings page loads and saves API keys
# ‚úì Model selection dropdown populates
# ‚úì API calls use selected models
# ‚úì Tooltips are clickable and accessible
# ‚úì Service worker registers without errors

# Test pages available:
# - test-calculations-updated.html (statistical tests)
# - test-model-selection.html (BYOK model selection)
# - test-markdown.html (markdown parsing edge cases)
# - test-llm-flow.html (complete LLM response flow)
# - test-gemini-api.html (direct Gemini API testing)
```

### Code Quality
```bash
# Backend code formatting
uv run black app/ tests/

# Sort imports
uv run isort app/ tests/

# Lint code
uv run flake8 app/ tests/

# Type checking
uv run mypy app/
```

## Project Structure

### Backend Structure (`app/`)
```
app/
‚îú‚îÄ‚îÄ main.py                 # FastAPI application entry point
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ config.py          # Settings and configuration
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ validate.py        # /validate/setup endpoint
‚îÇ   ‚îî‚îÄ‚îÄ analyze.py         # /analyze/results endpoint
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ requests.py        # Pydantic request models
‚îÇ   ‚îî‚îÄ‚îÄ responses.py       # Pydantic response models
‚îú‚îÄ‚îÄ statistics/
‚îÇ   ‚îî‚îÄ‚îÄ calculations.py    # Statistical calculations (TO BE PORTED)
‚îî‚îÄ‚îÄ llm/
    ‚îú‚îÄ‚îÄ base.py            # LLM provider interface
    ‚îú‚îÄ‚îÄ gemini.py          # Google Gemini provider
    ‚îú‚îÄ‚îÄ anthropic_client.py # Anthropic Claude provider
    ‚îú‚îÄ‚îÄ manager.py         # LLM manager with fallback
    ‚îî‚îÄ‚îÄ prompts.py         # LLM prompt templates
```

### Standalone Chrome Extension Structure (`chrome-extension-standalone/`)
```
chrome-extension-standalone/
‚îú‚îÄ‚îÄ manifest.json          # Extension configuration (Manifest V3)
‚îú‚îÄ‚îÄ popup.html            # Main popup interface with tabs
‚îú‚îÄ‚îÄ popup.js              # UI event handling and form logic
‚îú‚îÄ‚îÄ popup.css             # Modern styling with CSS variables
‚îú‚îÄ‚îÄ options.html          # Settings and API key management
‚îú‚îÄ‚îÄ options.js            # Model selection and preferences
‚îú‚îÄ‚îÄ options.css           # Settings page styling
‚îú‚îÄ‚îÄ background.js         # Service worker for lifecycle
‚îú‚îÄ‚îÄ shared.js             # Global utilities and constants
‚îú‚îÄ‚îÄ statistics.js         # Statistical engine (pure JS)
‚îú‚îÄ‚îÄ llm-client.js         # LLM API integration (BYOK)
‚îî‚îÄ‚îÄ assets/
    ‚îî‚îÄ‚îÄ icons/            # Extension icons (16, 48, 128px)
```

## API Endpoints (Current v1.0)

### POST /validate/setup
Analyzes proposed experiment setup for statistical feasibility.

**Request Structure**:
```json
{
  "hypothesis": "string",
  "metric": {
    "name": "string",
    "baseline_conversion_rate": 0.05
  },
  "parameters": {
    "minimum_detectable_effect_relative": 0.1,  // OR
    "minimum_detectable_effect_absolute": 0.005,
    "statistical_power": 0.8,
    "significance_level": 0.05,
    "variants": 2
  },
  "traffic": {
    "estimated_daily_users": 10000
  }
}
```

**Response**: Sample size per variant, test duration, trade-off matrix, AI hypothesis assessment

### POST /analyze/results
Interprets experiment results with actionable recommendations.

**Request Structure**:
```json
{
  "context": {
    "hypothesis": "string",
    "primary_metric_name": "string",
    "pm_notes": "optional string"
  },
  "results_data": {
    "variants": [
      {
        "name": "Control",
        "users": 5000,
        "conversions": 250
      },
      {
        "name": "Treatment",
        "users": 5000,
        "conversions": 300
      }
    ]
  }
}
```

**Response**: Statistical summary, p-value, confidence intervals, AI interpretation, recommendations

## Environment Configuration

Copy `.env.example` to `.env` and configure:

### API Keys
- `GOOGLE_API_KEY`: Google Gemini API key
- `ANTHROPIC_API_KEY`: Anthropic Claude API key

### Model Configuration
- `GEMINI_MODEL`: Gemini model name (default: "gemini-2.5-flash")
- `ANTHROPIC_MODEL`: Claude model name (default: "claude-sonnet-4-20250514")

### LLM Settings
- `DEFAULT_LLM_PROVIDER`: "gemini" or "anthropic"
- `LLM_FALLBACK_ENABLED`: true/false

### Testing LLM Connectivity
```bash
# Check LLM provider status
curl http://localhost:8000/llm/status

# Test with specific models
GEMINI_MODEL=gemini-1.5-flash uv run uvicorn app.main:app --reload
```

## Technical Architecture

- **FastAPI**: Modern Python web framework with automatic API docs
- **Pydantic**: Data validation and settings management
- **SciPy/StatsModels**: Statistical calculations and power analysis
- **Google Generative AI**: Gemini integration with fallback support
- **Anthropic**: Claude API integration with fallback support
- **UV**: Fast Python package manager for dependency management

## Chrome Extension Features

### üöÄ Recent Updates (June 2025)

#### Auto-Scroll to Results
- Results automatically scroll into view after API responses
- Prevents "hidden results" confusion
- Subtle animation with blue border highlight
- Scroll-to-top buttons (üîù) for easy navigation

#### Simplified Settings
- Removed technical configuration from UI
- Environment auto-detection (local/staging/production)
- Focus on user preferences only
- Zero configuration setup

#### PM-Friendly Interface
- Clickable tooltips with smart positioning
- Helpful, humorous guidance throughout
- Professional copy with relatable scenarios
- Full accessibility support (ARIA, keyboard nav)

### Known Issues
- **AI Follow-up Questions**: Markdown rendering issue where questions appear as run-on text
- **Pattern**: `**Category:** content **Next Category:** content`
- **Status**: Partial fix attempted, needs debugging of actual AI response format

## Key Statistical Concepts

### Core Calculations (Implemented in Client-Side JavaScript)

#### Sample Size Calculation
```javascript
// Two-proportion z-test formula (corrected implementation)
const variance1 = p1 * (1 - p1);
const variance2 = p2 * (1 - p2);
const n = Math.pow(zAlpha + zBeta, 2) * (variance1 + variance2) / Math.pow(delta, 2);

// Where:
// - zAlpha = Z-score for significance level (e.g., 1.96 for 0.05)
// - zBeta = Z-score for statistical power (e.g., 0.84 for 0.80)
// - delta = |p2 - p1| (absolute difference between proportions)
// - p1, p2 = Control and treatment conversion rates
```

#### Statistical Significance Testing
```javascript
// Two-proportion z-test with continuity correction
const pooledP = (conversions1 + conversions2) / (n1 + n2);
const se = Math.sqrt(pooledP * (1 - pooledP) * (1/n1 + 1/n2));

// Apply Yates' continuity correction for small samples
const continuityCorrection = Math.min(0.5 * (1/n1 + 1/n2), Math.abs(p1 - p2));
const zScore = (Math.abs(p1 - p2) - continuityCorrection) / se;
const pValue = 2 * (1 - normalCDF(Math.abs(zScore)));
```

#### Hart's Algorithm for Normal CDF
```javascript
// High-precision approximation of normal CDF
function normalCDF(x) {
  const a1 = 0.0705230784, a2 = 0.0422820123, a3 = 0.0092705272;
  const a4 = 0.0001520143, a5 = 0.0002765672, a6 = 0.0000430638;
  const absX = Math.abs(x);
  const t = 1 / (1 + 0.33267 * absX);
  // ... (see statistics.js for full implementation)
}
```

### Key Concepts
- **MDE (Minimum Detectable Effect)**: Can be relative (%) or absolute
- **Trade-off Matrix**: Shows duration vs. different MDEs (50%, 75%, 100%, 125%, 150%)
- **Statistical Power**: Default 0.80 (80% chance of detecting true effect)
- **Significance Level**: Default 0.05 (5% false positive rate)
- **Sample Size**: Users needed per variant for reliable results
- **Test Duration**: Days needed based on traffic

## LLM Integration

The API uses a fallback system between Google Gemini and Anthropic Claude:
1. Attempts to use the configured default provider
2. Falls back to alternative provider if the first fails
3. Provides static fallback responses if all LLM calls fail

### Current Models
- **Gemini**: `gemini-2.5-flash` (latest stable)
- **Claude**: `claude-sonnet-4-20250514` (Claude Sonnet 4)

### Model Features
- **Hypothesis Assessment**: AI-powered clarity scoring and improvement suggestions
- **Results Interpretation**: Plain-English explanation of statistical results
- **Next Steps**: Actionable recommendations with confidence levels
- **Follow-up Questions**: Strategic questions for deeper analysis

## Core Architecture Principles

- **Headless API-only**: No GUI, pure REST API
- **Stateless**: No data persistence, user accounts, or authentication in V1
- **Educational**: Provides plain-English explanations alongside statistical calculations
- **Context-aware**: Combines quantitative analysis with qualitative PM insights using LLM
- **Resilient**: LLM fallback ensures API remains functional even if AI services are unavailable

## Docker Deployment

The PM Tools API is containerized for production deployment. Docker provides consistent deployment across environments.

### Docker Build and Run

```bash
# Build the Docker image
docker build -t pmtools-api:latest .

# Run container locally for testing
docker run -d \
  --name pmtools-api \
  -p 8000:8000 \
  -e API_DEBUG=false \
  -e LOG_LEVEL=info \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e ANTHROPIC_API_KEY=your_anthropic_api_key \
  pmtools-api:latest

# Check container health
curl http://localhost:8000/health
```

### Production Deployment with Docker Compose

```bash
# Deploy using docker-compose (recommended for production)
docker-compose up -d

# Check logs
docker-compose logs -f pmtools-api

# Stop deployment
docker-compose down
```

### Coolify Deployment

This API is optimized for deployment on [Coolify](https://coolify.io/) - a self-hosted alternative to Heroku/Vercel.

**Step 1: Repository Setup**
- Push your code to a Git repository (GitHub/GitLab)
- Ensure `Dockerfile` and `docker-compose.yml` are in the root

**Step 2: Coolify Configuration**
1. Create new application in Coolify
2. Connect your Git repository 
3. Set build pack to "Docker Compose"
4. Configure environment variables as secrets:
   - `GOOGLE_API_KEY` (required)
   - `ANTHROPIC_API_KEY` (required)
   - `API_DEBUG=false`
   - `LOG_LEVEL=info`

**Step 3: Health Check Setup**
- Coolify will automatically use the health check defined in docker-compose.yml
- Health endpoint: `/health`
- Expected response: `{"status": "healthy"}`

### Docker Environment Variables

All configuration is handled via environment variables:

```bash
# Core API Settings
API_HOST=0.0.0.0              # Container host binding
API_PORT=8000                 # Container port
API_DEBUG=false               # Disable docs in production
LOG_LEVEL=info                # Logging level: debug, info, warning, error
WORKERS=4                     # Uvicorn worker processes

# LLM Configuration
DEFAULT_LLM_PROVIDER=gemini           # Primary LLM provider
LLM_FALLBACK_ENABLED=true            # Enable fallback to secondary provider
GEMINI_MODEL=gemini-2.5-flash        # Google Gemini model
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022  # Anthropic Claude model

# API Keys (set as secrets in Coolify)
GOOGLE_API_KEY=your_key_here         # Required for Gemini
ANTHROPIC_API_KEY=your_key_here      # Required for Claude
```

### Container Features

**Security**:
- Non-root user (`pmtools`) for container security
- Minimal attack surface with slim Python base image
- No unnecessary packages or tools in production image

**Performance**:
- UV package manager for fast dependency installation
- Multi-worker Uvicorn for production workloads
- Resource limits configurable via docker-compose

**Monitoring**:
- Built-in health checks for container orchestration
- Structured logging for production debugging
- LLM provider status endpoint (`/llm/status`)

## Troubleshooting

### Docker Issues
```bash
# Build image with no cache
docker build --no-cache -t pmtools-api:latest .

# Check container logs
docker logs pmtools-api

# Debug container interactively
docker run -it --entrypoint /bin/bash pmtools-api:latest

# Test container health
docker exec pmtools-api curl -f http://localhost:8000/health

# Remove all containers and images for clean restart
docker system prune -a
```

### LLM Issues
```bash
# Check provider status
curl http://localhost:8000/llm/status

# Common errors and solutions:
# 1. "404 models/gemini-pro is not found" - Update GEMINI_MODEL to gemini-2.5-flash
# 2. "Invalid API key" - Check GOOGLE_API_KEY/ANTHROPIC_API_KEY in .env
# 3. "No LLM providers available" - Ensure at least one valid API key is configured
# 4. "Rate limit exceeded" - Wait or switch to different provider
```

### Development Issues
```bash
# Clear Python cache
find . -name "*.pyc" -delete
find . -name "__pycache__" -delete

# Reinstall dependencies
uv sync --dev

# Check API health
curl http://localhost:8000/health
```

### Chrome Extension Issues (Updated June 2025)
```bash
# Extension not updating after changes
# 1. Go to chrome://extensions/
# 2. Click refresh icon on PM Tools extension
# 3. Clear extension storage if needed:
#    Right-click icon ‚Üí Options ‚Üí Clear all data

# Form submission error: "Variant users must be an integer >= 1"
# Cause: Missing name attributes on form inputs
# Solution: Ensure all inputs have name="fieldName" attributes
# Example: <input type="number" id="controlUsers" name="controlUsers">
# Fixed in: popup.html (all form inputs now have name attributes)

# Service worker registration failed (Status code: 15)
# Error: "Cannot read properties of undefined (reading 'onAlarm')"
# Cause: Missing 'alarms' permission in manifest.json
# Solution: Add "alarms" to permissions array in manifest.json
# Fixed: Added alarms permission for periodic maintenance tasks

# Gemini API errors (June 2025)
# Error: "Cannot read properties of undefined (reading '0')"
# Cause: Response structure validation missing
# Solution: Add comprehensive error checking before accessing nested properties
# Updates:
#   - Default model: gemini-2.5-flash (was gemini-2.0-flash-exp)
#   - Added responseMimeType: 'text/plain' to generation config
#   - Increased maxOutputTokens: 4096 (was 1024)
#   - Handle MAX_TOKENS finish reason as normal completion

# LLM output not rendering properly (Fixed December 2024)
# Symptoms: Raw markdown like **text** and *text* showing
# Root Causes: 
#   - LLM generating non-standard markdown patterns
#   - Incomplete bold markers (e.g., "3/10**")
#   - Headers with trailing bold markers (e.g., "Strengths:**")
#   - Orphaned bold markers on separate lines
# Solution: Enhanced formatLLMResponse() with preprocessing
# New Features:
#   - Pre-processing step to fix LLM-specific patterns
#   - Handles incomplete bold markers at line ends
#   - Fixes headers with trailing bold markers
#   - Supports horizontal rules (---) 
#   - Multi-line bold section handling
#   - Improved list processing with proper nesting
# Supports:
#   - Headers: # to ###### 
#   - Bold: **text** (including edge cases)
#   - Italic: *text* (with proper isolation from bold)
#   - Lists: *, -, ‚Ä¢ for bullets; 1. 2. for numbered
#   - Code: `inline code`
#   - Horizontal rules: ---
#   - Mixed formatting within lists
#   - Proper paragraph and line break handling

# Model selection not working
# 1. Check API key is valid in settings
# 2. Verify model fetch endpoints are accessible
# 3. Check browser console for API errors
# 4. Clear model cache in storage to force refresh
# Note: Model lists are cached for 24 hours to reduce API calls
```

## ‚úÖ Standalone Chrome Extension Implementation Details

### Statistical Engine Improvements

1. **Fixed Sample Size Formula**: The original implementation used a simplified formula that was inaccurate. Now uses the proper two-proportion z-test formula with separate variances.

2. **Hart's Algorithm**: Implemented high-precision normal CDF approximation instead of the error function approximation, achieving accuracy to ~10^-7.

3. **Yates' Continuity Correction**: Added for small sample sizes to improve accuracy when sample sizes are < 1000 per variant.

4. **Proper Z-Score Calculation**: Fixed the inverse normal CDF calculation for determining z-scores from probability values.

### BYOK Model Selection Feature

1. **Dynamic Model Discovery**:
   ```javascript
   // Gemini models endpoint
   GET https://generativelanguage.googleapis.com/v1beta/models?key={API_KEY}
   
   // Anthropic models endpoint  
   GET https://api.anthropic.com/v1/models
   Headers: x-api-key: {API_KEY}
   ```

2. **Model Caching Strategy**:
   - 24-hour TTL to reduce API calls
   - Cached in Chrome local storage
   - Manual refresh button in settings
   - Fallback to default models if fetch fails

3. **Model Selection UI**:
   - Dropdown populated with available models
   - Shows model capabilities (token limits, etc.)
   - Persists selection across sessions
   - Graceful fallback if selected model becomes unavailable

### PM-Focused Prompt Engineering

1. **Hypothesis Analysis** now includes:
   - Business impact estimation
   - Resource requirements (engineering days, design work)
   - Stakeholder mapping
   - Revenue/cost implications
   - Alignment with company OKRs

2. **Results Interpretation** provides:
   - Clear ship/no-ship decision
   - Practical vs statistical significance
   - Risk assessment by user segment
   - Specific next steps with owners
   - Strategic follow-up questions

### Technical Solutions to Common Issues

1. **FormData Serialization**:
   - Problem: Form inputs returned null values
   - Root cause: Missing `name` attributes
   - Solution: Added name="fieldName" to all inputs
   - Learning: FormData requires name attributes, not just IDs

2. **Service Worker Lifecycle**:
   - Problem: Registration failures
   - Root cause: Invalid event listeners and keepAlive patterns
   - Solution: Simplified to standard Manifest V3 patterns
   - Learning: Service workers have different lifecycle than persistent backgrounds

3. **Markdown Rendering**:
   - Problem: Raw markdown in LLM responses
   - Solution: Custom formatLLMResponse() function
   - Features: Handles bold, lists, line breaks
   - Sanitizes HTML to prevent XSS

### Performance Optimizations

1. **Calculation Speed**: All statistical calculations complete in < 10ms
2. **LLM Response Time**: 2-5 seconds with retry logic
3. **Model List Caching**: Reduces API calls by 95%
4. **Form Auto-save**: Debounced to prevent excessive storage writes

### Security Considerations

1. **API Key Storage**: Uses Chrome's secure storage API
2. **No External Dependencies**: Pure JavaScript implementation
3. **Content Security Policy**: Strict CSP in manifest
4. **Input Sanitization**: All user inputs sanitized before display

### API Response Handling (June 2025)

#### Gemini 2.5 Response Structure
```javascript
{
  "candidates": [{
    "content": {
      "parts": [{
        "text": "Generated response text"
      }],
      "role": "model"
    },
    "finishReason": "STOP" | "MAX_TOKENS" | "SAFETY",
    "index": 0
  }],
  "usageMetadata": {
    "promptTokenCount": 267,
    "candidatesTokenCount": 334,
    "totalTokenCount": 1289,
    "thoughtsTokenCount": 688  // New in Gemini 2.5
  },
  "modelVersion": "gemini-2.5-flash"
}
```

#### Response Validation
The extension now validates every level of the response structure:
1. Check if `candidates` array exists and has length
2. Validate `candidate.content` exists
3. Ensure `candidate.content.parts` is an array with items
4. Verify `parts[0].text` exists and is a string
5. Handle various `finishReason` values appropriately

### Markdown Rendering Implementation

The `formatLLMResponse()` function in `shared.js` provides comprehensive markdown parsing:

#### Supported Markdown
- **Headers**: `# H1` through `###### H6`
- **Bold text**: `**text**` ‚Üí `<strong>text</strong>`
- **Italic text**: `*text*` ‚Üí `<em>text</em>`
- **Inline code**: `` `code` `` ‚Üí `<code>code</code>`
- **Bullet lists**: `*`, `-`, or `‚Ä¢` at line start
- **Numbered lists**: `1.`, `2.`, etc. at line start
- **Paragraph handling**: Automatic `<p>` wrapping with proper spacing

#### Implementation Details
1. **Order matters**: Bold parsing before italic to prevent conflicts
2. **Negative lookbehind/ahead**: Prevents single asterisks from matching within double asterisks
3. **List grouping**: Consecutive list items are wrapped in `<ul>` or `<ol>`
4. **Smart paragraphs**: Doesn't wrap existing block elements

### Test Files

The extension includes test files for debugging:

#### test-gemini-api.html
- Tests direct API calls to Gemini
- Verifies API key functionality
- Shows detailed response structure
- Helps debug connection issues

#### test-markdown.html
- Tests markdown parsing functionality
- Includes complex test cases for all edge cases
- Visual verification of formatting
- Console logging for debugging
- Test cases include:
  - Standard bold/italic formatting
  - Headers with lists
  - Broken bold patterns from actual LLM output
  - Orphaned bold markers
  - Horizontal rules
  - Multi-line bold sections

#### test-llm-flow.html
- Tests the complete LLM response processing flow
- Simulates real hypothesis analysis and results interpretation
- Shows raw response, parsed sections, and final rendered output
- Includes test for broken format from user's example
- Useful for debugging the entire parsing pipeline

### June 2025 Updates Summary

1. **Gemini API Updates**:
   - Default model: `gemini-2.5-flash`
   - Token limit: 4096 (increased from 1024)
   - New `thoughtsTokenCount` in response metadata
   - `responseMimeType: 'text/plain'` in generation config

2. **Bug Fixes**:
   - Added `alarms` permission to manifest.json
   - Fixed form inputs with name attributes
   - Comprehensive API response validation
   - Improved markdown rendering with bold/italic support

3. **Performance**:
   - 24-hour model list caching
   - Increased token limits for complete responses
   - Better error messages for debugging

## Development Workflow

### Working on Both Components
```bash
# Terminal 1: Run API server
uv run uvicorn app.main:app --reload

# Terminal 2: Test API
curl http://localhost:8000/health

# Browser: Load Chrome extension
# chrome://extensions/ ‚Üí Reload PM Tools

# Test end-to-end flow
# 1. Open extension popup
# 2. Fill validate form
# 3. Submit and verify results
# 4. Check both frontend and backend logs
```

### Best Practices
1. **Maintain Calculation Accuracy**: Statistical formulas are critical
2. **PM-First Design**: Every feature should help PMs make better decisions
3. **Clear Error Messages**: Help PMs understand what went wrong
4. **Progressive Enhancement**: Core features work without LLM
5. **Accessibility**: Full keyboard and screen reader support