# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

PM Tools is a comprehensive A/B testing solution for Product Managers, consisting of two integrated components:

1. **FastAPI Backend** (`app/`) - Statistical calculations and LLM-powered analysis
2. **Chrome Extension** (`chrome-extension/`) - Browser-based UI for instant access

The project serves as a "statistical consultant in a box" providing:
- **Pre-Experiment Feasibility Calculator** - Validates experiment design before running tests
- **Post-Experiment Results Interpreter** - Analyzes results with AI-powered insights

### ğŸ¯ Standalone Chrome Extension (COMPLETED - December 2024)
The standalone Chrome extension is now fully implemented with:
- âœ… All statistical calculations running client-side in JavaScript
- âœ… BYOK (Bring Your Own Key) model with dynamic model selection
- âœ… No server dependencies - works entirely in the browser
- âœ… Direct API calls to Gemini and Anthropic
- âœ… Enhanced statistical accuracy with proper formulas
- âœ… PM-focused AI prompts for actionable insights
- âœ… Real-time model discovery from API providers
- âœ… 24-hour caching for model lists
- âœ… Markdown to HTML conversion for LLM responses

## Architecture Overview

### Current Architecture (v1.0)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chrome Extension   â”‚ <-----> â”‚   FastAPI Backend    â”‚
â”‚  (UI + API Client)  â”‚  HTTP   â”‚ (Stats + LLM + API)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Standalone Chrome Extension Architecture (v2.0 - LIVE)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Standalone Chrome Extension        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  UI Layer (popup.html/js)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Statistics Engine (JS)      â”‚    â”‚
â”‚  â”‚  - Sample size calculation   â”‚    â”‚
â”‚  â”‚  - Significance testing      â”‚    â”‚
â”‚  â”‚  - Hart's algorithm (CDF)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  LLM Client (BYOK)           â”‚    â”‚
â”‚  â”‚  - Dynamic model selection   â”‚    â”‚
â”‚  â”‚  - Direct API calls          â”‚    â”‚
â”‚  â”‚  - Fallback handling         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development Commands

### Backend (FastAPI) Commands

#### Setup and Installation
```bash
# Install dependencies
uv sync

# Install development dependencies
uv sync --dev
```

#### Running the API Server
```bash
# Run development server with auto-reload
uv run uvicorn app.main:app --reload

# Run on specific host/port
uv run uvicorn app.main:app --host 0.0.0.0 --port 8000

# Run in production mode
uv run uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Chrome Extension Commands

#### Installation (Standalone Version)
```bash
# 1. Open Chrome and navigate to:
chrome://extensions/

# 2. Enable "Developer mode" (top right)

# 3. Click "Load unpacked" and select:
/path/to/pmtools/chrome-extension-standalone/

# 4. Pin the extension to toolbar for easy access

# 5. Click extension icon â†’ Settings (âš™ï¸) to add API keys
```

#### Development Workflow
```bash
# 1. Edit extension files
# 2. Go to chrome://extensions/
# 3. Click refresh icon on PM Tools extension
# 4. Test changes in extension popup

# For background script debugging:
# chrome://extensions/ â†’ PM Tools â†’ "Inspect views: service worker"

# For popup debugging:
# Right-click extension icon â†’ "Inspect popup"
```

### Testing

#### Backend Tests
```bash
# Run all tests
uv run pytest

# Run tests with coverage
uv run pytest --cov=app

# Run specific test file
uv run pytest tests/test_statistics.py

# Run specific test
uv run pytest tests/test_api.py::TestValidateSetupEndpoint::test_valid_setup_request_relative_mde
```

#### Chrome Extension Testing
```bash
# Manual testing checklist:
# âœ“ Extension loads without errors
# âœ“ Form inputs have name attributes (required for FormData)
# âœ“ Form validation catches errors
# âœ“ Statistical calculations are accurate
# âœ“ Results display correctly with formatted markdown
# âœ“ Auto-scroll to results works
# âœ“ Export functionality works (CSV/JSON)
# âœ“ Settings page loads and saves API keys
# âœ“ Model selection dropdown populates
# âœ“ API calls use selected models
# âœ“ Tooltips are clickable and accessible
# âœ“ Service worker registers without errors

# Test pages available:
# - test-calculations-updated.html (statistical tests)
# - test-model-selection.html (BYOK model selection)
```

### Code Quality
```bash
# Backend code formatting
uv run black app/ tests/

# Sort imports
uv run isort app/ tests/

# Lint code
uv run flake8 app/ tests/

# Type checking
uv run mypy app/
```

## Project Structure

### Backend Structure (`app/`)
```
app/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ core/
â”‚   â””â”€â”€ config.py          # Settings and configuration
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ validate.py        # /validate/setup endpoint
â”‚   â””â”€â”€ analyze.py         # /analyze/results endpoint
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ requests.py        # Pydantic request models
â”‚   â””â”€â”€ responses.py       # Pydantic response models
â”œâ”€â”€ statistics/
â”‚   â””â”€â”€ calculations.py    # Statistical calculations (TO BE PORTED)
â””â”€â”€ llm/
    â”œâ”€â”€ base.py            # LLM provider interface
    â”œâ”€â”€ gemini.py          # Google Gemini provider
    â”œâ”€â”€ anthropic_client.py # Anthropic Claude provider
    â”œâ”€â”€ manager.py         # LLM manager with fallback
    â””â”€â”€ prompts.py         # LLM prompt templates
```

### Standalone Chrome Extension Structure (`chrome-extension-standalone/`)
```
chrome-extension-standalone/
â”œâ”€â”€ manifest.json          # Extension configuration (Manifest V3)
â”œâ”€â”€ popup.html            # Main popup interface with tabs
â”œâ”€â”€ popup.js              # UI event handling and form logic
â”œâ”€â”€ popup.css             # Modern styling with CSS variables
â”œâ”€â”€ options.html          # Settings and API key management
â”œâ”€â”€ options.js            # Model selection and preferences
â”œâ”€â”€ options.css           # Settings page styling
â”œâ”€â”€ background.js         # Service worker for lifecycle
â”œâ”€â”€ shared.js             # Global utilities and constants
â”œâ”€â”€ statistics.js         # Statistical engine (pure JS)
â”œâ”€â”€ llm-client.js         # LLM API integration (BYOK)
â””â”€â”€ assets/
    â””â”€â”€ icons/            # Extension icons (16, 48, 128px)
```

## API Endpoints (Current v1.0)

### POST /validate/setup
Analyzes proposed experiment setup for statistical feasibility.

**Request Structure**:
```json
{
  "hypothesis": "string",
  "metric": {
    "name": "string",
    "baseline_conversion_rate": 0.05
  },
  "parameters": {
    "minimum_detectable_effect_relative": 0.1,  // OR
    "minimum_detectable_effect_absolute": 0.005,
    "statistical_power": 0.8,
    "significance_level": 0.05,
    "variants": 2
  },
  "traffic": {
    "estimated_daily_users": 10000
  }
}
```

**Response**: Sample size per variant, test duration, trade-off matrix, AI hypothesis assessment

### POST /analyze/results
Interprets experiment results with actionable recommendations.

**Request Structure**:
```json
{
  "context": {
    "hypothesis": "string",
    "primary_metric_name": "string",
    "pm_notes": "optional string"
  },
  "results_data": {
    "variants": [
      {
        "name": "Control",
        "users": 5000,
        "conversions": 250
      },
      {
        "name": "Treatment",
        "users": 5000,
        "conversions": 300
      }
    ]
  }
}
```

**Response**: Statistical summary, p-value, confidence intervals, AI interpretation, recommendations

## Environment Configuration

Copy `.env.example` to `.env` and configure:

### API Keys
- `GOOGLE_API_KEY`: Google Gemini API key
- `ANTHROPIC_API_KEY`: Anthropic Claude API key

### Model Configuration
- `GEMINI_MODEL`: Gemini model name (default: "gemini-2.5-flash")
- `ANTHROPIC_MODEL`: Claude model name (default: "claude-sonnet-4-20250514")

### LLM Settings
- `DEFAULT_LLM_PROVIDER`: "gemini" or "anthropic"
- `LLM_FALLBACK_ENABLED`: true/false

### Testing LLM Connectivity
```bash
# Check LLM provider status
curl http://localhost:8000/llm/status

# Test with specific models
GEMINI_MODEL=gemini-1.5-flash uv run uvicorn app.main:app --reload
```

## Technical Architecture

- **FastAPI**: Modern Python web framework with automatic API docs
- **Pydantic**: Data validation and settings management
- **SciPy/StatsModels**: Statistical calculations and power analysis
- **Google Generative AI**: Gemini integration with fallback support
- **Anthropic**: Claude API integration with fallback support
- **UV**: Fast Python package manager for dependency management

## Chrome Extension Features

### ğŸš€ Recent Updates (June 2025)

#### Auto-Scroll to Results
- Results automatically scroll into view after API responses
- Prevents "hidden results" confusion
- Subtle animation with blue border highlight
- Scroll-to-top buttons (ğŸ”) for easy navigation

#### Simplified Settings
- Removed technical configuration from UI
- Environment auto-detection (local/staging/production)
- Focus on user preferences only
- Zero configuration setup

#### PM-Friendly Interface
- Clickable tooltips with smart positioning
- Helpful, humorous guidance throughout
- Professional copy with relatable scenarios
- Full accessibility support (ARIA, keyboard nav)

### Known Issues
- **AI Follow-up Questions**: Markdown rendering issue where questions appear as run-on text
- **Pattern**: `**Category:** content **Next Category:** content`
- **Status**: Partial fix attempted, needs debugging of actual AI response format

## Key Statistical Concepts

### Core Calculations (Implemented in Client-Side JavaScript)

#### Sample Size Calculation
```javascript
// Two-proportion z-test formula (corrected implementation)
const variance1 = p1 * (1 - p1);
const variance2 = p2 * (1 - p2);
const n = Math.pow(zAlpha + zBeta, 2) * (variance1 + variance2) / Math.pow(delta, 2);

// Where:
// - zAlpha = Z-score for significance level (e.g., 1.96 for 0.05)
// - zBeta = Z-score for statistical power (e.g., 0.84 for 0.80)
// - delta = |p2 - p1| (absolute difference between proportions)
// - p1, p2 = Control and treatment conversion rates
```

#### Statistical Significance Testing
```javascript
// Two-proportion z-test with continuity correction
const pooledP = (conversions1 + conversions2) / (n1 + n2);
const se = Math.sqrt(pooledP * (1 - pooledP) * (1/n1 + 1/n2));

// Apply Yates' continuity correction for small samples
const continuityCorrection = Math.min(0.5 * (1/n1 + 1/n2), Math.abs(p1 - p2));
const zScore = (Math.abs(p1 - p2) - continuityCorrection) / se;
const pValue = 2 * (1 - normalCDF(Math.abs(zScore)));
```

#### Hart's Algorithm for Normal CDF
```javascript
// High-precision approximation of normal CDF
function normalCDF(x) {
  const a1 = 0.0705230784, a2 = 0.0422820123, a3 = 0.0092705272;
  const a4 = 0.0001520143, a5 = 0.0002765672, a6 = 0.0000430638;
  const absX = Math.abs(x);
  const t = 1 / (1 + 0.33267 * absX);
  // ... (see statistics.js for full implementation)
}
```

### Key Concepts
- **MDE (Minimum Detectable Effect)**: Can be relative (%) or absolute
- **Trade-off Matrix**: Shows duration vs. different MDEs (50%, 75%, 100%, 125%, 150%)
- **Statistical Power**: Default 0.80 (80% chance of detecting true effect)
- **Significance Level**: Default 0.05 (5% false positive rate)
- **Sample Size**: Users needed per variant for reliable results
- **Test Duration**: Days needed based on traffic

## LLM Integration

The API uses a fallback system between Google Gemini and Anthropic Claude:
1. Attempts to use the configured default provider
2. Falls back to alternative provider if the first fails
3. Provides static fallback responses if all LLM calls fail

### Current Models
- **Gemini**: `gemini-2.5-flash` (latest stable)
- **Claude**: `claude-sonnet-4-20250514` (Claude Sonnet 4)

### Model Features
- **Hypothesis Assessment**: AI-powered clarity scoring and improvement suggestions
- **Results Interpretation**: Plain-English explanation of statistical results
- **Next Steps**: Actionable recommendations with confidence levels
- **Follow-up Questions**: Strategic questions for deeper analysis

## Core Architecture Principles

- **Headless API-only**: No GUI, pure REST API
- **Stateless**: No data persistence, user accounts, or authentication in V1
- **Educational**: Provides plain-English explanations alongside statistical calculations
- **Context-aware**: Combines quantitative analysis with qualitative PM insights using LLM
- **Resilient**: LLM fallback ensures API remains functional even if AI services are unavailable

## Docker Deployment

The PM Tools API is containerized for production deployment. Docker provides consistent deployment across environments.

### Docker Build and Run

```bash
# Build the Docker image
docker build -t pmtools-api:latest .

# Run container locally for testing
docker run -d \
  --name pmtools-api \
  -p 8000:8000 \
  -e API_DEBUG=false \
  -e LOG_LEVEL=info \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e ANTHROPIC_API_KEY=your_anthropic_api_key \
  pmtools-api:latest

# Check container health
curl http://localhost:8000/health
```

### Production Deployment with Docker Compose

```bash
# Deploy using docker-compose (recommended for production)
docker-compose up -d

# Check logs
docker-compose logs -f pmtools-api

# Stop deployment
docker-compose down
```

### Coolify Deployment

This API is optimized for deployment on [Coolify](https://coolify.io/) - a self-hosted alternative to Heroku/Vercel.

**Step 1: Repository Setup**
- Push your code to a Git repository (GitHub/GitLab)
- Ensure `Dockerfile` and `docker-compose.yml` are in the root

**Step 2: Coolify Configuration**
1. Create new application in Coolify
2. Connect your Git repository 
3. Set build pack to "Docker Compose"
4. Configure environment variables as secrets:
   - `GOOGLE_API_KEY` (required)
   - `ANTHROPIC_API_KEY` (required)
   - `API_DEBUG=false`
   - `LOG_LEVEL=info`

**Step 3: Health Check Setup**
- Coolify will automatically use the health check defined in docker-compose.yml
- Health endpoint: `/health`
- Expected response: `{"status": "healthy"}`

### Docker Environment Variables

All configuration is handled via environment variables:

```bash
# Core API Settings
API_HOST=0.0.0.0              # Container host binding
API_PORT=8000                 # Container port
API_DEBUG=false               # Disable docs in production
LOG_LEVEL=info                # Logging level: debug, info, warning, error
WORKERS=4                     # Uvicorn worker processes

# LLM Configuration
DEFAULT_LLM_PROVIDER=gemini           # Primary LLM provider
LLM_FALLBACK_ENABLED=true            # Enable fallback to secondary provider
GEMINI_MODEL=gemini-2.5-flash        # Google Gemini model
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022  # Anthropic Claude model

# API Keys (set as secrets in Coolify)
GOOGLE_API_KEY=your_key_here         # Required for Gemini
ANTHROPIC_API_KEY=your_key_here      # Required for Claude
```

### Container Features

**Security**:
- Non-root user (`pmtools`) for container security
- Minimal attack surface with slim Python base image
- No unnecessary packages or tools in production image

**Performance**:
- UV package manager for fast dependency installation
- Multi-worker Uvicorn for production workloads
- Resource limits configurable via docker-compose

**Monitoring**:
- Built-in health checks for container orchestration
- Structured logging for production debugging
- LLM provider status endpoint (`/llm/status`)

## Troubleshooting

### Docker Issues
```bash
# Build image with no cache
docker build --no-cache -t pmtools-api:latest .

# Check container logs
docker logs pmtools-api

# Debug container interactively
docker run -it --entrypoint /bin/bash pmtools-api:latest

# Test container health
docker exec pmtools-api curl -f http://localhost:8000/health

# Remove all containers and images for clean restart
docker system prune -a
```

### LLM Issues
```bash
# Check provider status
curl http://localhost:8000/llm/status

# Common errors and solutions:
# 1. "404 models/gemini-pro is not found" - Update GEMINI_MODEL to gemini-2.5-flash
# 2. "Invalid API key" - Check GOOGLE_API_KEY/ANTHROPIC_API_KEY in .env
# 3. "No LLM providers available" - Ensure at least one valid API key is configured
# 4. "Rate limit exceeded" - Wait or switch to different provider
```

### Development Issues
```bash
# Clear Python cache
find . -name "*.pyc" -delete
find . -name "__pycache__" -delete

# Reinstall dependencies
uv sync --dev

# Check API health
curl http://localhost:8000/health
```

### Chrome Extension Issues
```bash
# Extension not updating after changes
# 1. Go to chrome://extensions/
# 2. Click refresh icon on PM Tools extension
# 3. Clear extension storage if needed:
#    Right-click icon â†’ Options â†’ Clear all data

# Form submission error: "Variant users must be an integer >= 1"
# Cause: Missing name attributes on form inputs
# Solution: Ensure all inputs have name="fieldName" attributes
# Example: <input type="number" id="controlUsers" name="controlUsers">

# Service worker registration failed (Status code: 15)
# Cause: Problematic lifecycle management code
# Solution: Remove keepAlive() and non-existent event listeners
# Check: chrome://extensions/ â†’ "service worker" should show as active

# LLM output not rendering properly
# Cause: Markdown syntax displayed as raw text
# Solution: Use formatLLMResponse() function in shared.js
# Debug: Check console for LLM response format

# Model selection not working
# 1. Check API key is valid in settings
# 2. Verify model fetch endpoints are accessible
# 3. Check browser console for API errors
# 4. Clear model cache in storage to force refresh
```

## âœ… Standalone Chrome Extension Implementation Details

### Statistical Engine Improvements

1. **Fixed Sample Size Formula**: The original implementation used a simplified formula that was inaccurate. Now uses the proper two-proportion z-test formula with separate variances.

2. **Hart's Algorithm**: Implemented high-precision normal CDF approximation instead of the error function approximation, achieving accuracy to ~10^-7.

3. **Yates' Continuity Correction**: Added for small sample sizes to improve accuracy when sample sizes are < 1000 per variant.

4. **Proper Z-Score Calculation**: Fixed the inverse normal CDF calculation for determining z-scores from probability values.

### BYOK Model Selection Feature

1. **Dynamic Model Discovery**:
   ```javascript
   // Gemini models endpoint
   GET https://generativelanguage.googleapis.com/v1beta/models?key={API_KEY}
   
   // Anthropic models endpoint  
   GET https://api.anthropic.com/v1/models
   Headers: x-api-key: {API_KEY}
   ```

2. **Model Caching Strategy**:
   - 24-hour TTL to reduce API calls
   - Cached in Chrome local storage
   - Manual refresh button in settings
   - Fallback to default models if fetch fails

3. **Model Selection UI**:
   - Dropdown populated with available models
   - Shows model capabilities (token limits, etc.)
   - Persists selection across sessions
   - Graceful fallback if selected model becomes unavailable

### PM-Focused Prompt Engineering

1. **Hypothesis Analysis** now includes:
   - Business impact estimation
   - Resource requirements (engineering days, design work)
   - Stakeholder mapping
   - Revenue/cost implications
   - Alignment with company OKRs

2. **Results Interpretation** provides:
   - Clear ship/no-ship decision
   - Practical vs statistical significance
   - Risk assessment by user segment
   - Specific next steps with owners
   - Strategic follow-up questions

### Technical Solutions to Common Issues

1. **FormData Serialization**:
   - Problem: Form inputs returned null values
   - Root cause: Missing `name` attributes
   - Solution: Added name="fieldName" to all inputs
   - Learning: FormData requires name attributes, not just IDs

2. **Service Worker Lifecycle**:
   - Problem: Registration failures
   - Root cause: Invalid event listeners and keepAlive patterns
   - Solution: Simplified to standard Manifest V3 patterns
   - Learning: Service workers have different lifecycle than persistent backgrounds

3. **Markdown Rendering**:
   - Problem: Raw markdown in LLM responses
   - Solution: Custom formatLLMResponse() function
   - Features: Handles bold, lists, line breaks
   - Sanitizes HTML to prevent XSS

### Performance Optimizations

1. **Calculation Speed**: All statistical calculations complete in < 10ms
2. **LLM Response Time**: 2-5 seconds with retry logic
3. **Model List Caching**: Reduces API calls by 95%
4. **Form Auto-save**: Debounced to prevent excessive storage writes

### Security Considerations

1. **API Key Storage**: Uses Chrome's secure storage API
2. **No External Dependencies**: Pure JavaScript implementation
3. **Content Security Policy**: Strict CSP in manifest
4. **Input Sanitization**: All user inputs sanitized before display

## Development Workflow

### Working on Both Components
```bash
# Terminal 1: Run API server
uv run uvicorn app.main:app --reload

# Terminal 2: Test API
curl http://localhost:8000/health

# Browser: Load Chrome extension
# chrome://extensions/ â†’ Reload PM Tools

# Test end-to-end flow
# 1. Open extension popup
# 2. Fill validate form
# 3. Submit and verify results
# 4. Check both frontend and backend logs
```

### Best Practices
1. **Maintain Calculation Accuracy**: Statistical formulas are critical
2. **PM-First Design**: Every feature should help PMs make better decisions
3. **Clear Error Messages**: Help PMs understand what went wrong
4. **Progressive Enhancement**: Core features work without LLM
5. **Accessibility**: Full keyboard and screen reader support